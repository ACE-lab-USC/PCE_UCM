{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808f80c5-9846-452c-ab54-5081c921faff",
   "metadata": {},
   "source": [
    "# **Necessary libraries**\n",
    "### load necessary libraries for one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a7d2f-9b72-4163-9868-e0dcf90eead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e835c-34d9-4789-88c4-f71ca052856e",
   "metadata": {},
   "source": [
    "# **Parameters Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad824d86-84e9-48da-8303-5675e23b4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "    latitude_min (float):     Minimum latitude for filtering.\n",
    "    latitude_max (float):     Maximum latitude for filtering.\n",
    "    longitude_min (float):    Minimum longitude for filtering.\n",
    "    longitude_max (float):    Maximum longitude for filtering.\n",
    "    start_time (str):         Start of the date range in 'YYYY-MM-DD HH:MM' format.\n",
    "    end_time (str):           End of the date range in 'YYYY-MM-DD HH:MM' format.\n",
    "\"\"\"\n",
    "\n",
    "start_time='2019-07-03 00:00:00'\n",
    "end_time='2019-07-17 00:00:00'\n",
    "longitude_min=-119.04\n",
    "longitude_max=-116.28\n",
    "latitude_min=33.261\n",
    "latitude_max=34.75\n",
    "time_difference=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd6f1c3-3d50-47ce-8d43-9d7ddb6233f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for input and output CSV files\n",
    "AQS_path = '/project/zhan248_1326/hhao4018/Model Evaluation/hourly_TEMP_2019/hourly_TEMP_2019.csv'\n",
    "\n",
    "#1: only use the urban area stations\n",
    "#0: no open\n",
    "Use_Urban_Land = 1\n",
    "\n",
    "ref_file_path = \"/project/zhan248_1326/hhao4018/Model Evaluation/wrfout_d02_2016-08-10_10_00_00\"\n",
    "\n",
    "filtered_file_path = '/project/zhan248_1326/hhao4018/Model Evaluation/AQS_obs_T2.csv'\n",
    "\n",
    "# Define the path to the WRF output folder\n",
    "wrf_output_folder = '/project2/zhan248_1326/hhao4018/WRFv4.6.1_Modified_UQ/test/WRF_PCE_UQ_07_LA/'\n",
    "\n",
    "file_path_obs = filtered_file_path\n",
    "\n",
    "pairing_data_path = '/project/zhan248_1326/hhao4018/Model Evaluation/model_obs_pairs_T2.csv'\n",
    "\n",
    "picture_save_path=\"1 Benchmark Evaluation Result/Winter_T2_v2.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d280c-1cee-4d34-8bc3-94842323ebd0",
   "metadata": {},
   "source": [
    "# **Process Start**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4863b8-8399-4b9a-90be-fc1ff8cf0c48",
   "metadata": {},
   "source": [
    "### **Step 0: Using Urban Area Shift Urban obs stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ea02e-0496-4aa5-b4f5-5a8158d058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_landuse_type(target_lons, target_lats, file_path=ref_file_path):\n",
    "    dataset = nc.Dataset(file_path)\n",
    "    landuse = dataset.variables[\"LU_INDEX\"][:]\n",
    "    lat = dataset.variables[\"XLAT\"][:]\n",
    "    lon = dataset.variables[\"XLONG\"][:]\n",
    "    \n",
    "    landuse_types = []\n",
    "    for target_lon, target_lat in zip(target_lons, target_lats):\n",
    "        distance = np.sqrt((lat - target_lat)**2 + (lon - target_lon)**2)\n",
    "        index = np.unravel_index(np.argmin(distance), distance.shape)\n",
    "        \n",
    "        landuse_types.append(landuse[index])\n",
    "    \n",
    "    return landuse_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f89ba-2dc0-4ce8-8edf-836f99a036e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def get_unique_coords(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    coords = list(set(zip(df['Longitude'], df['Latitude'])))\n",
    "    return coords\n",
    "\n",
    "def filter_coordinates_by_landuse_and_shp(csv_file, ref_file_path, shp_file):\n",
    "    coords = get_unique_coords(csv_file)\n",
    "    lons, lats = zip(*coords)\n",
    "\n",
    "    landuse_types = get_landuse_type(list(lons), list(lats), file_path=ref_file_path)\n",
    "\n",
    "    gdf = gpd.read_file(shp_file)\n",
    "    shape_union = gdf.unary_union\n",
    "\n",
    "    filtered_coords = []\n",
    "    for (lon, lat), lu in zip(coords, landuse_types):\n",
    "        pt = Point(lon, lat)\n",
    "        if (lu > 50 or lu == 13) and shape_union.contains(pt):\n",
    "            filtered_coords.append((lon, lat))\n",
    "\n",
    "    return filtered_coords\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filtered_coordinates = filter_coordinates_by_landuse_and_shp(AQS_path, ref_file_path, 'southcoastAirB_ExportFeature/southcoastAirB_ExportFeature.shp')\n",
    "    print(filtered_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a38e5-732f-40bc-a170-43fcfd3c541f",
   "metadata": {},
   "source": [
    "### **Step 1: Read and shift AQS data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61f35a-f3df-4bb5-94d2-90d310cd944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shift_obs(AQS_path, filtered_file_path, filtered_coordinates, Use_Urban_Land):\n",
    "    \"\"\"\n",
    "    Load and filter AQS observation data based on spatial and temporal constraints.\n",
    "    \n",
    "    Parameters:\n",
    "        AQS_path (str):         Path to the input observation data file (CSV format).\n",
    "        filtered_file_path (str): Path to save the filtered observation data.\n",
    "    \"\"\"\n",
    "    filtered_chunks = []\n",
    "    \n",
    "    with open(AQS_path, 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for line in f) - 1 \n",
    "\n",
    "    chunksize = 10000\n",
    "    for chunk in tqdm(pd.read_csv(AQS_path, chunksize=chunksize), \n",
    "                      total=(total_lines // chunksize) + 1, \n",
    "                      desc=\"Processing data chunks\"):\n",
    "        chunk['Date Full'] = pd.to_datetime(chunk['Date GMT'] + ' ' + chunk['Time GMT'])\n",
    "        \n",
    "        if Use_Urban_Land == 0:    \n",
    "            filtered_chunk = chunk[\n",
    "                (chunk['Date Full'] >= start_time) &\n",
    "                (chunk['Date Full'] < end_time) &\n",
    "                (chunk['Longitude'] >= longitude_min) &\n",
    "                (chunk['Longitude'] <= longitude_max) &\n",
    "                (chunk['Latitude'] >= latitude_min) &\n",
    "                (chunk['Latitude'] <= latitude_max)\n",
    "            ]\n",
    "        else:\n",
    "            mask_coords = chunk.apply(lambda row: (row['Longitude'], row['Latitude']) in  filtered_coordinates, axis=1)\n",
    "\n",
    "            filtered_chunk = chunk[\n",
    "                (chunk['Date Full'] >= start_time) &\n",
    "                (chunk['Date Full'] < end_time) &\n",
    "                (chunk['Longitude'] >= longitude_min) &\n",
    "                (chunk['Longitude'] <= longitude_max) &\n",
    "                (chunk['Latitude'] >= latitude_min) &\n",
    "                (chunk['Latitude'] <= latitude_max) &\n",
    "                (mask_coords)\n",
    "            ]\n",
    "        \n",
    "        filtered_chunks.append(filtered_chunk)\n",
    "    \n",
    "    filtered_data = pd.concat(filtered_chunks, ignore_index=True)\n",
    "    \n",
    "    filtered_data.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Call the function to load and filter the observation data\n",
    "    load_shift_obs(AQS_path, filtered_file_path, filtered_coordinates, Use_Urban_Land)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa097b-a8fd-4050-b1e7-d1873c529b1d",
   "metadata": {},
   "source": [
    "### **Step 2: Read WRF-output data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec71c8-81e4-4101-a258-92248deb2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wrfout(wrf_output_folder):\n",
    "    \"\"\"\n",
    "    Reads WRF output files from a specified folder, extracting surface temperature (T2),\n",
    "    latitude (XLAT), and longitude (XLONG) data.\n",
    "    \n",
    "    Parameters:\n",
    "    wrf_output_folder (str): Path to the folder containing WRF output files.\n",
    "\n",
    "    Returns:\n",
    "    data_wrf_output (dict): Dictionary with filenames as keys and T2 data as values.\n",
    "    latitudes_model (np.ndarray): Array of latitude values (XLAT) from the first WRF file.\n",
    "    longitudes_model (np.ndarray): Array of longitude values (XLONG) from the first WRF file.\n",
    "    \"\"\"\n",
    "    # Dictionary to store T2 data from each file\n",
    "    data_wrf_output = {}\n",
    "    \n",
    "    # Initialize lat/long variables\n",
    "    latitudes_model, longitudes_model = None, None\n",
    "\n",
    "    # Get list of files and initialize progress bar\n",
    "    files = [f for f in os.listdir(wrf_output_folder) if f.startswith(\"wrfout_d02\")]\n",
    "    for filename in tqdm(files, desc=\"Processing WRF output files\"):\n",
    "        \n",
    "        # Construct full file path\n",
    "        sub_hour_path = os.path.join(wrf_output_folder, filename)\n",
    "        \n",
    "        # Open the WRF file as an xarray dataset\n",
    "        wrfout = xr.open_dataset(sub_hour_path, engine='netcdf4')\n",
    "        \n",
    "        # Store the T2 (surface temperature) data in the dictionary\n",
    "        data_wrf_output[filename] = wrfout['T2']\n",
    "        \n",
    "        # Load latitude and longitude only once from the first matching file\n",
    "        if latitudes_model is None or longitudes_model is None:\n",
    "            latitudes_model = wrfout['XLAT'].values\n",
    "            longitudes_model = wrfout['XLONG'].values\n",
    "\n",
    "    data_wrf_output['latitudes'] = latitudes_model\n",
    "    data_wrf_output['longitudes'] = longitudes_model\n",
    "    \n",
    "    return data_wrf_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Call the function to read WRF data\n",
    "    data_wrf_output = read_wrfout(wrf_output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa3c1e-c708-41a8-9ffa-44a5a9651140",
   "metadata": {},
   "source": [
    "### **Step 3: Match Obs data and model data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376832f-9179-488f-bc10-d9587cf1beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairing_obs_model(file_path_obs, data_wrf_output, pairing_data_path):\n",
    "    \"\"\"\n",
    "    Pair observed values with model values based on the nearest latitude and longitude grid points.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path_obs (str): Path to the CSV file containing observation data.\n",
    "    data_wrf_output (dict): Dictionary containing model data, including latitudes and longitudes.\n",
    "    pairing_data_path (str): Path to save the paired data output.\n",
    "    time_difference (int): Time difference in hours to adjust for LA local time.\n",
    "    \"\"\"\n",
    "    # Load observation data\n",
    "    df_obs = pd.read_csv(file_path_obs)\n",
    "\n",
    "    # Extract relevant columns\n",
    "    Datetime = pd.to_datetime(df_obs['Date Full'])\n",
    "    Hour = df_obs['Time GMT']\n",
    "    StationID = df_obs['Site Num']\n",
    "    StationName = df_obs['State Name']\n",
    "    AT = df_obs['Sample Measurement']\n",
    "    Lat = df_obs['Latitude']\n",
    "    Long = df_obs['Longitude']\n",
    "\n",
    "    # Initialize list to collect paired results\n",
    "    results = []\n",
    "\n",
    "    # Perform pairing of observations and model data with progress bar\n",
    "    for i in tqdm(range(len(Datetime)), desc=\"Pairing observations with model data\"):\n",
    "        target_lat, target_lon = Lat[i], Long[i]\n",
    "\n",
    "        # Calculate distances to find the nearest model grid point\n",
    "        distance = np.sqrt((data_wrf_output['latitudes'] - target_lat)**2 + \n",
    "                           (data_wrf_output['longitudes'] - target_lon)**2)\n",
    "        min_index = np.unravel_index(np.argmin(distance, axis=None), distance.shape)\n",
    "\n",
    "        month, day, hour = Datetime[i].month, Datetime[i].day, Datetime[i].hour\n",
    "        \n",
    "        # Retrieve model temperature at the nearest grid point and convert from Kelvin to Celsius\n",
    "        date_str = f\"wrfout_d02_2019-{month:02d}-{day:02d}_{hour:02d}:00:00\"\n",
    "        t2 = data_wrf_output[date_str].isel(south_north=min_index[1], west_east=min_index[2]).values[0] - 273.15\n",
    "\n",
    "        # Append paired observation and model data to results\n",
    "        results.append([StationID[i],StationName[i], Datetime[i], Hour[i], (AT[i]-32)/1.8, t2, min_index[1], min_index[2],Lat[i], Long[i]])\n",
    "\n",
    "    # Create DataFrame and save as CSV\n",
    "    df = pd.DataFrame(results, columns=['StationID','StationName', 'LA Datetime', 'LA_Hour', 'T2_Obs', 'T2_model', 'sn', 'we','latitude','longitude'])\n",
    "    df.to_csv(pairing_data_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    pairing_obs_model(file_path_obs, data_wrf_output, pairing_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c2bc1-2b57-473b-8ed1-0e2a2b0bb446",
   "metadata": {},
   "source": [
    "# **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b9e74-e1b6-458d-8399-be792c84d540",
   "metadata": {},
   "source": [
    "### **Step 1: common evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0b1dc-8925-4b3f-ac43-37012d9a5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate error metrics\n",
    "def calculate_metrics(true, pred):\n",
    "    \"\"\"\n",
    "    Calculates common error metrics to evaluate model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    true (array-like): Array of true observation values.\n",
    "    pred (array-like): Array of model-predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    list: List containing MB, MAE, RMSE, NMB, NME, and R_squared values.\n",
    "    \"\"\"\n",
    "    # Mean Bias (MB)\n",
    "    MB = np.mean(pred - true)\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = np.mean(np.abs(pred - true))\n",
    "    # Root Mean Square Error (RMSE)\n",
    "    RMSE = np.sqrt(np.mean((pred - true) ** 2))\n",
    "    # Normalized Mean Bias (NMB)\n",
    "    NMB = (np.sum(pred - true) / np.sum(true)) * 100\n",
    "    # Normalized Mean Error (NME)\n",
    "    NME = (np.sum(np.abs(pred - true)) / np.sum(true)) * 100\n",
    "    # Coefficient of Determination (R²)\n",
    "    R_squared = np.corrcoef(pred, true)[0, 1] ** 2\n",
    "    # Return all calculated error metrics\n",
    "    return [MB, MAE, RMSE, NMB, NME, R_squared]\n",
    "\n",
    "# Define a function to display error metrics\n",
    "def display_metrics(pairing_data_path, model_name):\n",
    "    \"\"\"\n",
    "    Loads paired data from a CSV file, calculates error metrics, \n",
    "    and displays the results.\n",
    "    \n",
    "    Parameters:\n",
    "    pairing_data_path (str): Path to the CSV file containing paired observed and model data.\n",
    "    model_name (str): Name of the model, used as a label in the results.\n",
    "    \"\"\"\n",
    "    # Load the CSV file containing paired observed and model data\n",
    "    df_model_obs_pairs = pd.read_csv(pairing_data_path)\n",
    "    df_model_obs_pairs = df_model_obs_pairs.dropna(subset=['T2_Obs', 'T2_model'])\n",
    "    # Convert T2_Obs and T2_model to numeric\n",
    "    df_model_obs_pairs['T2_Obs'] = pd.to_numeric(df_model_obs_pairs['T2_Obs'], errors='coerce')\n",
    "    df_model_obs_pairs['T2_model'] = pd.to_numeric(df_model_obs_pairs['T2_model'], errors='coerce')\n",
    "    cleaned_data = df_model_obs_pairs.dropna(subset=['T2_Obs', 'T2_model'])\n",
    "    \n",
    "    # Call the calculate_metrics function to compute error metrics\n",
    "    result = calculate_metrics(cleaned_data['T2_Obs'].values, cleaned_data['T2_model'].values)\n",
    "\n",
    "    \n",
    "    # Initialize a dictionary to store the metrics\n",
    "    metrics = {\n",
    "        'MB': [],       # Mean Bias\n",
    "        'MAE': [],      # Mean Absolute Error\n",
    "        'RMSE': [],     # Root Mean Square Error\n",
    "        'NMB (%)': [],  # Normalized Mean Bias (as a percentage)\n",
    "        'NME (%)': [],  # Normalized Mean Error (as a percentage)\n",
    "        'R²': []        # Coefficient of Determination\n",
    "    }\n",
    "    \n",
    "    # Populate the dictionary with calculated metric values\n",
    "    for i, key in enumerate(metrics):\n",
    "        metrics[key].append(result[i])\n",
    "        \n",
    "    # Convert metrics dictionary to DataFrame with the model name as the row index\n",
    "    results_df = pd.DataFrame(metrics, index=[model_name])\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    display(results_df)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    display_metrics(pairing_data_path, 'T2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffbf94-cb1a-4f62-8b67-b0cb9c7925cd",
   "metadata": {},
   "source": [
    "### **Step 2: Plot scatter between obs and model data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641ad96-6a4d-46b9-b9ff-ac1fd3ea7b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def plot_scatter(pairing_data_path, picture_save_path):\n",
    "    data = pd.read_csv(pairing_data_path)\n",
    "    data = data.dropna(subset=['T2_Obs', 'T2_model'])\n",
    "    \n",
    "    # 提取观测值和模型值\n",
    "    observed = data['T2_Obs'].values\n",
    "    model = data['T2_model'].values\n",
    "    \n",
    "    # 拟合线性模型\n",
    "    regressor = LinearRegression()\n",
    "    observed_reshaped = observed.reshape(-1, 1)\n",
    "    regressor.fit(observed_reshaped, model)\n",
    "    model_pred = regressor.predict(observed_reshaped)\n",
    "    slope = regressor.coef_[0]\n",
    "    intercept = regressor.intercept_\n",
    "\n",
    "    # 2) 计算统计指标\n",
    "    sim=model\n",
    "    obs=observed\n",
    "    mb   = np.mean(sim - obs)                                     # Mean Bias\n",
    "    mae  = mean_absolute_error(obs, sim)                          # MAE\n",
    "    rmse = np.sqrt(mean_squared_error(obs, sim))                  # RMSE\n",
    "    r2   = r2_score(obs, sim)      \n",
    "    \n",
    "    # 计算密度\n",
    "    xy = np.vstack([observed, model])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    idx = z.argsort()\n",
    "    observed, model, z = observed[idx], model[idx], z[idx]\n",
    "\n",
    "    # 绘制图表\n",
    "    mpl.rcParams['font.size'] = 24\n",
    "    mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    scatter = ax.scatter(observed, model, c=z, s=20, cmap='viridis', edgecolor=None)\n",
    "    \n",
    "    # 添加 y = x 线\n",
    "    plt.plot([0, 100], [0, 100], 'r--', label='Simulation Temperature = Observation Temperature', linewidth =3)\n",
    "\n",
    "    # 8) 注记统计指标\n",
    "    stats_text = (\n",
    "        f\"MB   = {mb:.2f} K\\n\"\n",
    "        f\"MAE  = {mae:.2f} K\\n\"\n",
    "        f\"RMSE = {rmse:.2f} K\\n\"\n",
    "        f\"$R^2$   = {r2:.2f}\"\n",
    "    )\n",
    "    ax.text(0.05, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=20,\n",
    "            va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "    # 设置图表细节\n",
    "    ax.set_xlabel(\"Observation Temperature (°C)\")\n",
    "    ax.set_ylabel(\"Simulation Temperature (°C)\")\n",
    "    ax.set_xlim(observed.min(),observed.max())\n",
    "    ax.set_ylim(observed.min(),observed.max())\n",
    "    ax.set_title(\"\")\n",
    "    #ax.legend(loc=\"upper left\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    #plt.savefig(picture_save_path, format='png', dpi=800)\n",
    "    plt.show()\n",
    "\n",
    "# 示范调用函数\n",
    "plot_scatter(pairing_data_path, picture_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
